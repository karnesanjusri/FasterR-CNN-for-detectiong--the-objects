{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHZOgoXExsU8"
      },
      "source": [
        "# Read This before getting started\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook (Colab) was created to simplify the process of training a Faster R-CNN model on a custom object detection dataset. It allows users to quickly train and evaluate the model on their dataset, focusing on generating test metrics like mAP scores. This notebook is designed to work with datasets in COCO format and provides an easy workflow for training a Faster R-CNN model.\n",
        "\n",
        "The goal is to help users avoid the complexities of setting up the environment, data preprocessing, model configuration, and evaluation steps, all in a single notebook.\n",
        "\n",
        "## Why This Notebook Was Created\n",
        "\n",
        "- **Simplified Workflow:** This notebook offers a streamlined way to train and evaluate Faster R-CNN on custom datasets.\n",
        "- **COCO Format Support:** It is built to work with datasets in the COCO format, one of the most popular formats for object detection tasks.\n",
        "- **Metrics Calculation:** The notebook automatically computes the mean Average Precision (mAP) score during testing, allowing users to assess model performance.\n",
        "\n",
        "## Dataset Format Guidelines\n",
        "\n",
        "### 1. **COCO Format**\n",
        "   - The dataset should be in the COCO format, which includes two main components: **Images** and **Annotations**.\n",
        "   - The images should be stored in a directory, and the annotations should be in a JSON file that links the images to their respective object annotations.\n",
        "   - Important: Make sure to include the full absolute path for all images in the JSON file (whether the images are stored locally or on a drive).\n",
        "\n",
        "### 2. **JSON File Structure Example**\n",
        "\n",
        "The JSON file should follow this format for object detection:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"images\": [\n",
        "    {\n",
        "      \"id\": 1,\n",
        "      \"file_name\": \"/path/to/image1.jpg\",  // Full absolute path to the image\n",
        "      \"height\": 720,\n",
        "      \"width\": 1280\n",
        "    },\n",
        "    {\n",
        "      \"id\": 2,\n",
        "      \"file_name\": \"/path/to/image2.jpg\",  // Full absolute path to the image\n",
        "      \"height\": 720,\n",
        "      \"width\": 1280\n",
        "    }\n",
        "  ],\n",
        "  \"annotations\": [\n",
        "    {\n",
        "      \"image_id\": 1,\n",
        "      \"category_id\": 1,\n",
        "      \"bbox\": [x_min, y_min, width, height],\n",
        "      \"area\": area,\n",
        "      \"iscrowd\": 0\n",
        "    },\n",
        "    {\n",
        "      \"image_id\": 1,\n",
        "      \"category_id\": 2,\n",
        "      \"bbox\": [x_min, y_min, width, height],\n",
        "      \"area\": area,\n",
        "      \"iscrowd\": 0\n",
        "    }\n",
        "  ],\n",
        "  \"categories\": [\n",
        "    {\n",
        "      \"id\": 1,\n",
        "      \"name\": \"class1\"\n",
        "    },\n",
        "    {\n",
        "      \"id\": 2,\n",
        "      \"name\": \"class2\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n",
        "```\n",
        "## 3. Important Notes\n",
        "\n",
        "### Category IDs:\n",
        "Ensure the class IDs start from 1 (not 0). The ID `0` is reserved for the background class.\n",
        "\n",
        "### Bounding Boxes:\n",
        "Annotations should include bounding box coordinates in the form `[x_min, y_min, width, height]`.\n",
        "\n",
        "### Category Names:\n",
        "List all the object classes in the `categories` section of the JSON.\n",
        "\n",
        "## 4. Example Directory Structure\n",
        "\n",
        "```\n",
        "dataset/\n",
        "├── images/\n",
        "│   ├── image1.jpg\n",
        "│   ├── image2.jpg\n",
        "└── annotations/\n",
        "    └── ann.json\n",
        "\n",
        "```\n",
        "## Training Guidelines\n",
        "\n",
        "### Batch Size Consideration:\n",
        "The batch size is an important parameter that influences GPU memory usage (VRAM).\n",
        "If Colab crashes due to insufficient VRAM, try reducing the batch size.\n",
        "For instance, reduce the batch size from 8 to 4 or 2 in the `create_dataloader` function.\n",
        "\n",
        "### Colab Crashes:\n",
        "If the Colab session crashes due to memory limits, restart the session.\n",
        "Lower the batch size and retry the training. This will help the model train without running out of memory.\n",
        "\n",
        "### Training Time:\n",
        "Depending on your dataset size and batch size, training could take several hours. Be patient or consider using a more powerful environment if needed.\n",
        "\n",
        "### Metrics Calculation:\n",
        "After each epoch, the notebook will evaluate the model and display the mean Average Precision (mAP) score for the validation dataset.\n",
        "Pay attention to these metrics to evaluate the performance of your model.\n",
        "\n",
        "### Adjusting Hyperparameters:\n",
        "If you wish to fine-tune the model, feel free to adjust hyperparameters like learning rate (`lr`), weight decay, and step size for the learning rate scheduler.\n",
        "\n",
        "## Final Notes\n",
        "\n",
        "This notebook was created by [**ady-cf**](https://github.com/ady-cf).\n",
        "Feel free to fork this notebook and adapt it for your own use.\n",
        "If you face any issues or need further assistance, please create an issue on the GitHub repository or reach out to [**ady-cf**](https://github.com/ady-cf).\n",
        "\n",
        "Enjoy training your Faster R-CNN model for object detection!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n14jDBT3rvE"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aox0w73_AuiO"
      },
      "source": [
        "## Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8M0XZQ7Aycq",
        "outputId": "fb3711e8-fda4-48cf-bad0-8be39aa3cc80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boykj-qJzbY_"
      },
      "source": [
        "## Download and Import Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACno859LqtAx"
      },
      "source": [
        "### Install libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS23dwe4AgCy",
        "outputId": "07a886c8-e001-46e7-d13b-ad012b608d8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchmetrics[detection]\n",
            "  Downloading torchmetrics-1.6.2-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics[detection]) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics[detection]) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics[detection]) (2.5.1+cu124)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics[detection])\n",
            "  Downloading lightning_utilities-0.14.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pycocotools>2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics[detection]) (2.0.8)\n",
            "Requirement already satisfied: torchvision>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics[detection]) (0.20.1+cu124)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (75.1.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (4.12.2)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools>2.0.0->torchmetrics[detection]) (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[detection]) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[detection]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[detection]) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[detection]) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->torchmetrics[detection])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->torchmetrics[detection])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->torchmetrics[detection])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->torchmetrics[detection])\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->torchmetrics[detection])\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->torchmetrics[detection])\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->torchmetrics[detection])\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->torchmetrics[detection])\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->torchmetrics[detection])\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[detection]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[detection]) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->torchmetrics[detection])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[detection]) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[detection]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics[detection]) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.15.1->torchmetrics[detection]) (11.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics[detection]) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.17.0)\n",
            "Downloading lightning_utilities-0.14.0-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.6.2-py3-none-any.whl (931 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m931.6/931.6 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchmetrics-1.6.2\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics[detection]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGTEbJu2qQIP"
      },
      "source": [
        "### Download Helper Scripts for Object Detection\n",
        "\n",
        "This cell downloads essential helper scripts from the PyTorch Vision repository, specifically for object detection tasks. These scripts provide functionalities for training, evaluation, and data transformation, particularly when working with the COCO dataset.\n",
        "\n",
        "For a detailed tutorial on object detection with PyTorch and Torchvision, refer to this link: [PyTorch Object Detection Tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n",
        "\n",
        "**Scripts Downloaded:**\n",
        "\n",
        "* **`engine.py`:** Contains functions for training and evaluation loops.\n",
        "* **`utils.py`:** Provides general utility functions used in object detection.\n",
        "* **`coco_utils.py`:** Contains utility functions specific to the COCO dataset.\n",
        "* **`coco_eval.py`:** Implements evaluation metrics for COCO-style datasets.\n",
        "* **`transforms.py`:** Defines data transformations for image preprocessing.\n",
        "\n",
        "**Usage:**\n",
        "\n",
        "Run this cell to download the scripts directly into your Colab environment. These scripts are then available for import and use in subsequent cells of your notebook.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "* These scripts are directly from the `main` branch of the PyTorch Vision repository. Ensure you're using a compatible version of PyTorch and Torchvision.\n",
        "* After running this cell, you can import functions from these scripts like `from engine import train_one_epoch, evaluate` or `from utils import ...`.\n",
        "* These scripts are useful when working with object detection tasks, especially when your data uses the COCO annotation format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8Sr2bahZe64",
        "outputId": "29d4078d-249c-4ed6-c243-94b2e031056c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-wRxxJyZzeHA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import time\n",
        "import errno\n",
        "import math\n",
        "import random\n",
        "import copy\n",
        "import json\n",
        "import datetime\n",
        "import contextlib\n",
        "from collections import defaultdict, deque\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "from contextlib import redirect_stdout\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "# PyTorch and torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import torch.distributed as dist\n",
        "from torch import Tensor\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import ops, transforms as T\n",
        "from torchvision.io import read_image\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import mask_rcnn\n",
        "from torchvision.transforms.v2 import functional as F, InterpolationMode\n",
        "from torchvision.tv_tensors import Image, BoundingBoxes\n",
        "\n",
        "# Pycocotools\n",
        "from pycocotools import mask as coco_mask\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import pycocotools.mask as mask_util\n",
        "\n",
        "# Torchmetrics\n",
        "import torchmetrics\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/content\")\n",
        "import utils\n",
        "from coco_eval import CocoEvaluator, convert_to_xywh\n",
        "from coco_eval import CocoEvaluator\n",
        "from coco_utils import get_coco_api_from_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq8BkY83Z62J"
      },
      "source": [
        "## Faster-RCNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8F1E8kdr3ua"
      },
      "source": [
        "### Custom Training Functions\n",
        "\n",
        "This cell contains custom modifications of functions from `engine.py` used in PyTorch's object detection framework.\n",
        "\n",
        "## Functions\n",
        "\n",
        "1. **train_one_epoch**:\n",
        "   - This function trains the model for one epoch.\n",
        "   - It logs metrics, computes losses, and updates the optimizer.\n",
        "   - Supports automatic mixed precision (AMP) with `torch.amp.autocast` if a scaler is provided.\n",
        "   - This function is a modified version of the `train_one_epoch` function from `engine.py` for easier integration and use.\n",
        "\n",
        "2. **_get_iou_types**:\n",
        "   - This function retrieves the IoU (Intersection over Union) types used for evaluating the model.\n",
        "   - The function checks if the model is of type `MaskRCNN` or `KeypointRCNN` to include segmentations and keypoints, respectively.\n",
        "\n",
        "## Usage\n",
        "\n",
        "- This code assumes that you have a PyTorch object detection model, a dataset, and an optimizer set up.\n",
        "- The `train_one_epoch` function can be used to train the model for a single epoch. It requires the following parameters:\n",
        "  - `model`: The model to train (e.g., Faster R-CNN, Mask R-CNN).\n",
        "  - `optimizer`: The optimizer used for training.\n",
        "  - `lr_scheduler`: The learning rate scheduler (optional).\n",
        "  - `data_loader`: DataLoader for the training dataset.\n",
        "  - `device`: The device (CPU or GPU) where the model should be trained.\n",
        "  - `epoch`: The current epoch number.\n",
        "  - `print_freq`: Frequency at which to print progress.\n",
        "  - `scaler`: A `GradScaler` for mixed precision training (optional).\n",
        "\n",
        "## Modifications\n",
        "\n",
        "- These functions have been modified from the original `engine.py` to simplify their usage in a Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c6UmuRzcY6qo"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, optimizer,lr_scheduler,data_loader, device, epoch, print_freq, scaler=None):\n",
        "    model.train()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
        "    header = f\"Epoch: [{epoch}]\"\n",
        "\n",
        "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
        "        with torch.amp.autocast('cuda',enabled=scaler is not None):\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
        "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "\n",
        "        loss_value = losses_reduced.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(f\"Loss is {loss_value}, stopping training\")\n",
        "            print(loss_dict_reduced)\n",
        "            sys.exit(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        if scaler is not None:\n",
        "            scaler.scale(losses).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    return metric_logger\n",
        "\n",
        "\n",
        "def _get_iou_types(model):\n",
        "    model_without_ddp = model\n",
        "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
        "        model_without_ddp = model.module\n",
        "    iou_types = [\"bbox\"]\n",
        "    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n",
        "        iou_types.append(\"segm\")\n",
        "    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n",
        "        iou_types.append(\"keypoints\")\n",
        "    return iou_types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYy8EPBIskgB"
      },
      "source": [
        "### Custom Evaluation Functions\n",
        "\n",
        "This cell contains custom modifications for evaluating an object detection model in PyTorch, particularly for calculating mean Average Precision (mAP) scores.\n",
        "\n",
        "## Functions\n",
        "\n",
        "1. **evaluate**:\n",
        "   - This function evaluates the model on a given dataset and computes the mean Average Precision (mAP) at different IoU thresholds (0.5 and 0.5:0.95).\n",
        "   - It uses the `MeanAveragePrecision` evaluator from `torchmetrics` to calculate mAP scores.\n",
        "   - The function runs the model in inference mode, performs non-maximum suppression on the predictions, and compares them with ground truth boxes to compute the precision metrics.\n",
        "   - Supports both GPU and CPU-based inference.\n",
        "\n",
        "2. **print_metrics**:\n",
        "   - This function prints the evaluation results.\n",
        "   - It displays the overall mAP for both mAP@50 and mAP@50-95, as well as per-class mAP for each IoU threshold.\n",
        "   - It provides insights into the model's performance across different classes.\n",
        "\n",
        "## Usage\n",
        "\n",
        "- The `evaluate` function requires the following parameters:\n",
        "  - `model`: The trained model to evaluate (e.g., Faster R-CNN).\n",
        "  - `data_loader`: The DataLoader for the test/validation dataset.\n",
        "  - `device`: The device (CPU or GPU) where the model should be evaluated.\n",
        "  - `verbose`: A boolean flag for printing more detailed evaluation logs (default is `False`).\n",
        "\n",
        "- The `print_metrics` function is called after evaluating the model with `evaluate` to display the mAP scores.\n",
        "  - `results_map50`: The mAP@50 evaluation results.\n",
        "  - `results_map50_95`: The mAP@50-95 evaluation results.\n",
        "  - `CLASSES`: A list of class names corresponding to the labels in the dataset.\n",
        "\n",
        "## Modifications\n",
        "\n",
        "- These functions are based on the original `evaluate` and evaluation routines from  `engine.py`, with custom adjustments to improve usability and reporting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "adTYV6zDvSuQ"
      },
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def evaluate(model, data_loader, device, verbose=False):\n",
        "    n_threads = torch.get_num_threads()\n",
        "    torch.set_num_threads(1)\n",
        "\n",
        "    device = torch.device(device)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    header = \"Test:\"\n",
        "\n",
        "    evaluator_map50 = MeanAveragePrecision(iou_thresholds=[0.5], class_metrics=True, extended_summary=True)\n",
        "    evaluator_map50_95 = MeanAveragePrecision(class_metrics=True, extended_summary=True)\n",
        "\n",
        "    for images, target_batch in tqdm(metric_logger.log_every(data_loader, 100, header), total=len(data_loader)):\n",
        "        images = [img.to(device) for img in images]\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(images)\n",
        "\n",
        "        for i in range(len(images)):\n",
        "            gt_boxes = target_batch[i][\"boxes\"]\n",
        "            gt_labels = target_batch[i][\"labels\"]\n",
        "\n",
        "            valid_gt_idx = gt_labels != 0\n",
        "            gt_boxes = gt_boxes[valid_gt_idx]\n",
        "            gt_labels = gt_labels[valid_gt_idx]\n",
        "\n",
        "            pred_boxes = outputs[i][\"boxes\"].detach().cpu()\n",
        "            pred_scores = outputs[i][\"scores\"].detach().cpu()\n",
        "            pred_labels = outputs[i][\"labels\"].detach().cpu()\n",
        "\n",
        "            valid_pred_idx = (pred_labels != 0)\n",
        "            pred_boxes = pred_boxes[valid_pred_idx]\n",
        "            pred_scores = pred_scores[valid_pred_idx]\n",
        "            pred_labels = pred_labels[valid_pred_idx]\n",
        "\n",
        "            evaluator_map50.update(\n",
        "                preds=[{\"boxes\": pred_boxes, \"scores\": pred_scores, \"labels\": pred_labels}],\n",
        "                target=[{\"boxes\": gt_boxes, \"labels\": gt_labels}]\n",
        "            )\n",
        "\n",
        "            evaluator_map50_95.update(\n",
        "                preds=[{\"boxes\": pred_boxes, \"scores\": pred_scores, \"labels\": pred_labels}],\n",
        "                target=[{\"boxes\": gt_boxes, \"labels\": gt_labels}]\n",
        "            )\n",
        "\n",
        "    torch.set_num_threads(n_threads)\n",
        "\n",
        "    results_map50 = evaluator_map50.compute()\n",
        "    results_map50_95 = evaluator_map50_95.compute()\n",
        "\n",
        "    return results_map50, results_map50_95\n",
        "\n",
        "def print_metrics(results_map50, results_map50_95, CLASSES):\n",
        "\n",
        "    print(\"\\n\\n==== Validation Results (mAP@50-95) ====\")\n",
        "    print(f\"mAP50-95: {results_map50_95['map']:.3f}\")\n",
        "\n",
        "    print(\"\\nPer Class mAP (0.50:0.95 IoU threshold):\")\n",
        "\n",
        "    map_per_class_50_95 = results_map50_95[\"map_per_class\"].cpu().to_dense().numpy()\n",
        "    for i, map_value in enumerate(map_per_class_50_95):\n",
        "        class_name = CLASSES[i]\n",
        "        print(f\"{class_name}: {map_value:.3f}\")\n",
        "\n",
        "    print(\"\\n\\n==== Validation Results (mAP@50) ====\")\n",
        "    print(f\"mAP@50: {results_map50['map_50']:.3f}\")\n",
        "\n",
        "    print(\"\\nPer Class mAP (50% IoU threshold):\")\n",
        "\n",
        "    map_per_class_50 = results_map50[\"map_per_class\"].cpu().to_dense().numpy()\n",
        "    for i, map_value in enumerate(map_per_class_50):\n",
        "        class_name = CLASSES[i]\n",
        "        print(f\"{class_name}: {map_value:.3f}\")\n",
        "\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26YnKqODuEF-"
      },
      "source": [
        "### Custom COCO Dataset and Training Pipeline\n",
        "\n",
        "This cell defines a custom dataset class and the necessary functions for training a Faster R-CNN model using a COCO-style dataset.\n",
        "\n",
        "## Functions\n",
        "\n",
        "1. **CustomCocoDataset**:\n",
        "   - This is a custom dataset class that inherits from `torchvision.datasets.CocoDetection` to handle loading and processing of images and annotations.\n",
        "   - It extracts bounding box information, labels, areas, and crowd annotations for each object in an image.\n",
        "   - The class also handles transformations on the image and target, if provided.\n",
        "   - `__getitem__` returns the image and corresponding target dictionary, which includes the bounding boxes, labels, and other annotations.\n",
        "\n",
        "2. **get_coco_dataset**:\n",
        "   - This function initializes the `CustomCocoDataset` by passing the image directory and annotation file.\n",
        "   - It can be used to load the dataset for both training and testing.\n",
        "\n",
        "3. **collate_fn**:\n",
        "   - This function is used to combine a list of samples into a batch during data loading. It is passed as the `collate_fn` parameter when creating the `DataLoader`.\n",
        "\n",
        "4. **split_dataset**:\n",
        "   - This function splits the dataset into training and testing subsets based on the `train_fraction` parameter.\n",
        "   - By default, 90% of the data is used for training, and 10% for testing.\n",
        "   - You can change the `train_fraction` parameter to adjust the split ratio.\n",
        "   - The random split is done using a fixed seed (`SEED`) for reproducibility, but you can modify it as needed.\n",
        "\n",
        "5. **create_dataloader**:\n",
        "   - This function creates and returns the `DataLoader` instances for both training and testing datasets.\n",
        "   - The `batch_size_train` and `batch_size_test` parameters control the batch sizes for each dataset.\n",
        "\n",
        "6. **get_model**:\n",
        "   - This function initializes a Faster R-CNN model pre-trained on the COCO dataset and replaces its classifier head with a new one suitable for a custom number of classes.\n",
        "   - The `num_classes` parameter specifies the number of output classes (including the background).\n",
        "\n",
        "7. **setup_optimizer_scheduler**:\n",
        "   - This function sets up the optimizer (Adam) and learning rate scheduler (StepLR) for training.\n",
        "   - It returns the optimizer and scheduler, which can be used during the training loop.\n",
        "\n",
        "8. **train_and_evaluate**:\n",
        "   - This function handles the entire training and evaluation process for a specified number of epochs.\n",
        "   - It calls the `train_one_epoch` function for each epoch and evaluates the model on the test dataset using the `evaluate` function.\n",
        "   - It prints out evaluation metrics after each epoch using the `print_metrics` function.\n",
        "\n",
        "## How to Use\n",
        "\n",
        "1. To change the train/test split, adjust the `train_fraction` parameter in the `split_dataset` function. The default is 0.9 (90% for training and 10% for testing). To change this:\n",
        "   - Set a different value for `train_fraction` (e.g., `train_fraction=0.8` for 80% training and 20% testing).\n",
        "\n",
        "2. The training loop will automatically use the specified split for training and testing.\n",
        "\n",
        "3. Use the `train_and_evaluate` function to run the training and evaluation pipeline, where you can specify the number of epochs, device (CPU/GPU), and the list of class names (`CLASSES`).\n",
        "\n",
        "4. Modify `batch_size_train` and `batch_size_test` to adjust the batch sizes as needed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JN-Z9Ww1bAKl"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "class CustomCocoDataset(torchvision.datasets.CocoDetection):\n",
        "    def __init__(self, img_folder, ann_file, transforms=None):\n",
        "        super().__init__(img_folder, ann_file)\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, target = super().__getitem__(idx)\n",
        "        image_info = self.coco.loadImgs(self.ids[idx])[0]\n",
        "        image_path = os.path.join(self.root, image_info[\"file_name\"])\n",
        "\n",
        "        boxes, labels, areas, iscrowd = [], [], [], []\n",
        "        for obj in target:\n",
        "            x_min, y_min, width, height = obj[\"bbox\"]\n",
        "            x_max, y_max = x_min + width, y_min + height\n",
        "            boxes.append([x_min, y_min, x_max, y_max])\n",
        "            labels.append(obj[\"category_id\"])\n",
        "            areas.append(width * height)\n",
        "            iscrowd.append(obj.get(\"iscrowd\", 0))\n",
        "\n",
        "        if not boxes:\n",
        "            boxes = [[0, 0, 1, 1]]\n",
        "            labels = [0]\n",
        "            areas = [1.0]\n",
        "            iscrowd = [0]\n",
        "\n",
        "        image = T.ToTensor()(image)\n",
        "        target = {\n",
        "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
        "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
        "            \"image_id\": torch.tensor([idx], dtype=torch.int64),\n",
        "            \"area\": torch.tensor(areas, dtype=torch.float32),\n",
        "            \"iscrowd\": torch.tensor(iscrowd, dtype=torch.uint8),\n",
        "            \"image_path\": image_path,\n",
        "        }\n",
        "\n",
        "        if self.transforms:\n",
        "            image, target = self.transforms(image, target)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "def get_coco_dataset(img_dir, ann_file):\n",
        "    return CustomCocoDataset(img_dir, ann_file, transforms=None)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "def split_dataset(dataset, train_fraction=0.9, seed=SEED):\n",
        "    train_size = int(train_fraction * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    return random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(seed))\n",
        "\n",
        "def create_dataloader(train_dataset, test_dataset, batch_size_train=8, batch_size_test=4):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False, collate_fn=collate_fn)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "def get_model(num_classes):\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"COCO_V1\")\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "def setup_optimizer_scheduler(model, lr=0.0001, weight_decay=0.0005, step_size=5, gamma=0.5):\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
        "    lr_scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "    return optimizer, lr_scheduler\n",
        "\n",
        "def train_and_evaluate(model, train_loader, test_loader, optimizer, lr_scheduler, num_epochs, device, CLASSES):\n",
        "    for epoch in range(num_epochs):\n",
        "        train_one_epoch(model, optimizer, lr_scheduler, train_loader, device, epoch, print_freq=len(train_loader))\n",
        "        o1, o2 = evaluate(model, test_loader, device, verbose=True)\n",
        "        print_metrics(o1, o2, CLASSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4ag1G-Fn_Gi"
      },
      "source": [
        "### Object Detection Model Training and Evaluation\n",
        "\n",
        "This cell contains the code for training a Faster R-CNN model on a custom dataset for object detection. The dataset is expected to be in the COCO format, and the code includes steps for loading the dataset, training the model, and evaluating the results.\n",
        "\n",
        "Steps\n",
        "\n",
        "1. Dataset Paths\n",
        "   - The dataset consists of images and annotations stored in COCO format.\n",
        "   - The images are located in a specified directory, and the annotations are provided in a corresponding JSON file.\n",
        "   \n",
        "   Example Path:\n",
        "   - Image folder: \"/path/to/images\"\n",
        "   - Annotation file: \"/path/to/annotations.json\"\n",
        "\n",
        "2. Dataset and DataLoader\n",
        "   - The dataset is loaded using the `get_coco_dataset` function.\n",
        "   - The dataset is then split into training and testing subsets using the `split_dataset` function.\n",
        "   - Data is loaded in batches via the `create_dataloader` function, with configurable batch sizes for both training and testing.\n",
        "\n",
        "3. Model Setup\n",
        "   - A Faster R-CNN model pre-trained on a standard dataset (like COCO) is loaded via the `get_model` function.\n",
        "   - The model's final classifier is adjusted to output the correct number of classes based on the `num_classes` parameter (including the background class).\n",
        "   - The model is moved to the device (GPU if available, otherwise CPU).\n",
        "\n",
        "4. Optimizer and Scheduler\n",
        "   - The Adam optimizer is initialized with a learning rate and weight decay.\n",
        "   - A step learning rate scheduler (`StepLR`) is set up to decay the learning rate at specified intervals during training.\n",
        "\n",
        "5. Class Labels\n",
        "   - The classes to be detected (excluding the background) are specified in the `CLASSES` list.\n",
        "   - Update this list with the actual class names (e.g., \"Object1\", \"Object2\", \"Object3\", etc.).\n",
        "   \n",
        "   Example:\n",
        "   - `CLASSES = [\"Object1\", \"Object2\", \"Object3\"]`\n",
        "\n",
        "6. Training and Evaluation\n",
        "   - The `train_and_evaluate` function trains the model for a specified number of epochs and evaluates the model on the test dataset after each epoch.\n",
        "   - The model's performance metrics (e.g., mAP) are reported after each epoch.\n",
        "\n",
        "7. Training Complete\n",
        "   - After the training process completes, a message \"Training complete!\" is printed to indicate the process has finished.\n",
        "\n",
        "\n",
        "Modifications\n",
        "\n",
        "1. Dataset Split:\n",
        "   - The dataset is split into training and testing sets by default using a 90%/10% ratio. You can change the split by modifying the `train_fraction` parameter in the `split_dataset` function.\n",
        "   - Example: To use a 80%/20% split: `split_dataset(full_dataset, train_fraction=0.8)`.\n",
        "\n",
        "2. Class Names:\n",
        "   - Update the `CLASSES` list to match the classes in your dataset (excluding the background class).\n",
        "\n",
        "3. Number of Epochs:\n",
        "   - By default, the model will train for 10 epochs. You can adjust the number of epochs by modifying the `num_epochs` parameter in the `train_and_evaluate` function.\n",
        "\n",
        "4. Evaluation Metrics:\n",
        "   - During training, the evaluation metrics (e.g., mean Average Precision or mAP) are printed after each epoch. You can add more evaluation metrics if required.\n",
        "\n",
        "5. Optimizer and Scheduler:\n",
        "   - You can modify the learning rate (`lr`), weight decay, step size, and gamma values in the `setup_optimizer_scheduler` function to tune training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dITMhCPEnW7D",
        "outputId": "5362f791-0245-4b5c-a18d-e087299b9ef4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Epoch: [0]  [0/2]  eta: 0:00:04  lr: 0.000100  loss: 2.4693 (2.4693)  loss_classifier: 1.6049 (1.6049)  loss_box_reg: 0.0837 (0.0837)  loss_objectness: 0.4705 (0.4705)  loss_rpn_box_reg: 0.3102 (0.3102)  time: 2.1650  data: 0.9534  max mem: 5599\n",
            "Epoch: [0]  [1/2]  eta: 0:00:02  lr: 0.000100  loss: 1.5236 (1.9964)  loss_classifier: 0.4927 (1.0488)  loss_box_reg: 0.0837 (0.0895)  loss_objectness: 0.3907 (0.4306)  loss_rpn_box_reg: 0.3102 (0.4275)  time: 2.2352  data: 1.0272  max mem: 5599\n",
            "Epoch: [0] Total time: 0:00:04 (2.2364 s / it)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [00:01<00:01,  1.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [0/2]  eta: 0:00:03    time: 1.8985  data: 1.3272  max mem: 5599\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:03<00:00,  1.77s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [1/2]  eta: 0:00:01    time: 1.7719  data: 1.1882  max mem: 5599\n",
            "Test: Total time: 0:00:03 (1.7727 s / it)\n",
            "\n",
            "\n",
            "==== Validation Results (mAP@50-95) ====\n",
            "mAP50-95: 0.008\n",
            "\n",
            "Per Class mAP (0.50:0.95 IoU threshold):\n",
            "class1: 0.000\n",
            "class2: 0.000\n",
            "class3: 0.033\n",
            "class4: 0.000\n",
            "\n",
            "\n",
            "==== Validation Results (mAP@50) ====\n",
            "mAP@50: 0.034\n",
            "\n",
            "Per Class mAP (50% IoU threshold):\n",
            "class1: 0.000\n",
            "class2: 0.000\n",
            "class3: 0.135\n",
            "class4: 0.000\n",
            "\n",
            "\n",
            "Epoch: [1]  [0/2]  eta: 0:00:04  lr: 0.000100  loss: 0.8315 (0.8315)  loss_classifier: 0.3176 (0.3176)  loss_box_reg: 0.2305 (0.2305)  loss_objectness: 0.0166 (0.0166)  loss_rpn_box_reg: 0.2668 (0.2668)  time: 2.1545  data: 0.9711  max mem: 5599\n",
            "Epoch: [1]  [1/2]  eta: 0:00:02  lr: 0.000100  loss: 0.7191 (0.7753)  loss_classifier: 0.1761 (0.2469)  loss_box_reg: 0.0744 (0.1524)  loss_objectness: 0.0166 (0.0217)  loss_rpn_box_reg: 0.2668 (0.3543)  time: 2.2051  data: 1.0152  max mem: 5600\n",
            "Epoch: [1] Total time: 0:00:04 (2.2062 s / it)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [00:01<00:01,  1.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [0/2]  eta: 0:00:03    time: 1.5280  data: 0.9694  max mem: 5600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:03<00:00,  1.58s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [1/2]  eta: 0:00:01    time: 1.5773  data: 0.9998  max mem: 5600\n",
            "Test: Total time: 0:00:03 (1.5783 s / it)\n",
            "\n",
            "\n",
            "==== Validation Results (mAP@50-95) ====\n",
            "mAP50-95: 0.000\n",
            "\n",
            "Per Class mAP (0.50:0.95 IoU threshold):\n",
            "class1: 0.000\n",
            "class2: 0.000\n",
            "class3: 0.000\n",
            "class4: 0.000\n",
            "\n",
            "\n",
            "==== Validation Results (mAP@50) ====\n",
            "mAP@50: 0.000\n",
            "\n",
            "Per Class mAP (50% IoU threshold):\n",
            "class1: 0.000\n",
            "class2: 0.000\n",
            "class3: 0.000\n",
            "class4: 0.000\n",
            "\n",
            "\n",
            "Epoch: [2]  [0/2]  eta: 0:00:04  lr: 0.000050  loss: 0.7468 (0.7468)  loss_classifier: 0.3511 (0.3511)  loss_box_reg: 0.2020 (0.2020)  loss_objectness: 0.0074 (0.0074)  loss_rpn_box_reg: 0.1863 (0.1863)  time: 2.3925  data: 1.1922  max mem: 5600\n",
            "Epoch: [2]  [1/2]  eta: 0:00:02  lr: 0.000050  loss: 0.6804 (0.7136)  loss_classifier: 0.2710 (0.3110)  loss_box_reg: 0.1298 (0.1659)  loss_objectness: 0.0074 (0.0285)  loss_rpn_box_reg: 0.1863 (0.2081)  time: 2.4691  data: 1.2675  max mem: 5600\n",
            "Epoch: [2] Total time: 0:00:04 (2.4703 s / it)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [00:01<00:01,  1.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [0/2]  eta: 0:00:02    time: 1.4921  data: 0.9278  max mem: 5600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:03<00:00,  1.56s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [1/2]  eta: 0:00:01    time: 1.5633  data: 0.9822  max mem: 5600\n",
            "Test: Total time: 0:00:03 (1.5646 s / it)\n",
            "\n",
            "\n",
            "==== Validation Results (mAP@50-95) ====\n",
            "mAP50-95: 0.114\n",
            "\n",
            "Per Class mAP (0.50:0.95 IoU threshold):\n",
            "class1: 0.000\n",
            "class2: 0.065\n",
            "class3: 0.392\n",
            "class4: 0.000\n",
            "\n",
            "\n",
            "==== Validation Results (mAP@50) ====\n",
            "mAP@50: 0.218\n",
            "\n",
            "Per Class mAP (50% IoU threshold):\n",
            "class1: 0.000\n",
            "class2: 0.072\n",
            "class3: 0.799\n",
            "class4: 0.000\n",
            "\n",
            "\n",
            "Epoch: [3]  [0/2]  eta: 0:00:04  lr: 0.000050  loss: 0.4579 (0.4579)  loss_classifier: 0.1525 (0.1525)  loss_box_reg: 0.1391 (0.1391)  loss_objectness: 0.0730 (0.0730)  loss_rpn_box_reg: 0.0934 (0.0934)  time: 2.1572  data: 0.9573  max mem: 5600\n",
            "Epoch: [3]  [1/2]  eta: 0:00:02  lr: 0.000050  loss: 0.4579 (0.4734)  loss_classifier: 0.1525 (0.1532)  loss_box_reg: 0.1269 (0.1330)  loss_objectness: 0.0635 (0.0682)  loss_rpn_box_reg: 0.0934 (0.1190)  time: 2.2096  data: 1.0039  max mem: 5600\n",
            "Epoch: [3] Total time: 0:00:04 (2.2108 s / it)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [00:01<00:01,  1.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [0/2]  eta: 0:00:03    time: 1.6585  data: 1.0783  max mem: 5600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:03<00:00,  1.88s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [1/2]  eta: 0:00:01    time: 1.8778  data: 1.2685  max mem: 5600\n",
            "Test: Total time: 0:00:03 (1.8788 s / it)\n",
            "\n",
            "\n",
            "==== Validation Results (mAP@50-95) ====\n",
            "mAP50-95: 0.163\n",
            "\n",
            "Per Class mAP (0.50:0.95 IoU threshold):\n",
            "class1: 0.057\n",
            "class2: 0.113\n",
            "class3: 0.362\n",
            "class4: 0.119\n",
            "\n",
            "\n",
            "==== Validation Results (mAP@50) ====\n",
            "mAP@50: 0.338\n",
            "\n",
            "Per Class mAP (50% IoU threshold):\n",
            "class1: 0.143\n",
            "class2: 0.154\n",
            "class3: 0.780\n",
            "class4: 0.277\n",
            "\n",
            "\n",
            "Epoch: [4]  [0/2]  eta: 0:00:05  lr: 0.000050  loss: 0.4751 (0.4751)  loss_classifier: 0.1551 (0.1551)  loss_box_reg: 0.1553 (0.1553)  loss_objectness: 0.0892 (0.0892)  loss_rpn_box_reg: 0.0755 (0.0755)  time: 2.6340  data: 1.3855  max mem: 5600\n",
            "Epoch: [4]  [1/2]  eta: 0:00:02  lr: 0.000025  loss: 0.4751 (0.5021)  loss_classifier: 0.1551 (0.1581)  loss_box_reg: 0.1553 (0.1614)  loss_objectness: 0.0449 (0.0671)  loss_rpn_box_reg: 0.0755 (0.1156)  time: 2.4710  data: 1.2324  max mem: 5600\n",
            "Epoch: [4] Total time: 0:00:04 (2.4719 s / it)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [00:01<00:01,  1.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [0/2]  eta: 0:00:03    time: 1.5488  data: 0.9773  max mem: 5600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:03<00:00,  1.60s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [1/2]  eta: 0:00:01    time: 1.5982  data: 1.0094  max mem: 5600\n",
            "Test: Total time: 0:00:03 (1.5993 s / it)\n",
            "\n",
            "\n",
            "==== Validation Results (mAP@50-95) ====\n",
            "mAP50-95: 0.185\n",
            "\n",
            "Per Class mAP (0.50:0.95 IoU threshold):\n",
            "class1: 0.038\n",
            "class2: 0.145\n",
            "class3: 0.321\n",
            "class4: 0.235\n",
            "\n",
            "\n",
            "==== Validation Results (mAP@50) ====\n",
            "mAP@50: 0.371\n",
            "\n",
            "Per Class mAP (50% IoU threshold):\n",
            "class1: 0.125\n",
            "class2: 0.182\n",
            "class3: 0.712\n",
            "class4: 0.467\n",
            "\n",
            "\n",
            "Epoch: [5]  [0/2]  eta: 0:00:04  lr: 0.000025  loss: 0.4741 (0.4741)  loss_classifier: 0.1584 (0.1584)  loss_box_reg: 0.1924 (0.1924)  loss_objectness: 0.0329 (0.0329)  loss_rpn_box_reg: 0.0904 (0.0904)  time: 2.1511  data: 0.9475  max mem: 5600\n",
            "Epoch: [5]  [1/2]  eta: 0:00:02  lr: 0.000025  loss: 0.4741 (0.5080)  loss_classifier: 0.1576 (0.1580)  loss_box_reg: 0.1924 (0.1939)  loss_objectness: 0.0329 (0.0375)  loss_rpn_box_reg: 0.0904 (0.1186)  time: 2.4655  data: 1.2360  max mem: 5600\n",
            "Epoch: [5] Total time: 0:00:04 (2.4668 s / it)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [00:01<00:01,  1.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [0/2]  eta: 0:00:03    time: 1.5325  data: 0.9622  max mem: 5600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:03<00:00,  1.58s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [1/2]  eta: 0:00:01    time: 1.5818  data: 0.9953  max mem: 5600\n",
            "Test: Total time: 0:00:03 (1.5826 s / it)\n",
            "\n",
            "\n",
            "==== Validation Results (mAP@50-95) ====\n",
            "mAP50-95: 0.204\n",
            "\n",
            "Per Class mAP (0.50:0.95 IoU threshold):\n",
            "class1: 0.018\n",
            "class2: 0.206\n",
            "class3: 0.372\n",
            "class4: 0.219\n",
            "\n",
            "\n",
            "==== Validation Results (mAP@50) ====\n",
            "mAP@50: 0.348\n",
            "\n",
            "Per Class mAP (50% IoU threshold):\n",
            "class1: 0.091\n",
            "class2: 0.250\n",
            "class3: 0.724\n",
            "class4: 0.329\n",
            "\n",
            "\n",
            "Epoch: [6]  [0/2]  eta: 0:00:04  lr: 0.000025  loss: 0.4597 (0.4597)  loss_classifier: 0.1637 (0.1637)  loss_box_reg: 0.2140 (0.2140)  loss_objectness: 0.0049 (0.0049)  loss_rpn_box_reg: 0.0771 (0.0771)  time: 2.1683  data: 0.9572  max mem: 5600\n",
            "Epoch: [6]  [1/2]  eta: 0:00:02  lr: 0.000025  loss: 0.4597 (0.4843)  loss_classifier: 0.1637 (0.1666)  loss_box_reg: 0.2140 (0.2188)  loss_objectness: 0.0046 (0.0048)  loss_rpn_box_reg: 0.0771 (0.0942)  time: 2.2237  data: 1.0060  max mem: 5600\n",
            "Epoch: [6] Total time: 0:00:04 (2.2248 s / it)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [00:01<00:01,  1.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [0/2]  eta: 0:00:03    time: 1.5687  data: 0.9958  max mem: 5600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:03<00:00,  1.73s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [1/2]  eta: 0:00:01    time: 1.7313  data: 1.1253  max mem: 5600\n",
            "Test: Total time: 0:00:03 (1.7322 s / it)\n",
            "\n",
            "\n",
            "==== Validation Results (mAP@50-95) ====\n",
            "mAP50-95: 0.204\n",
            "\n",
            "Per Class mAP (0.50:0.95 IoU threshold):\n",
            "class1: 0.033\n",
            "class2: 0.188\n",
            "class3: 0.348\n",
            "class4: 0.247\n",
            "\n",
            "\n",
            "==== Validation Results (mAP@50) ====\n",
            "mAP@50: 0.384\n",
            "\n",
            "Per Class mAP (50% IoU threshold):\n",
            "class1: 0.083\n",
            "class2: 0.250\n",
            "class3: 0.794\n",
            "class4: 0.408\n",
            "\n",
            "\n",
            "Epoch: [7]  [0/2]  eta: 0:00:05  lr: 0.000013  loss: 0.4742 (0.4742)  loss_classifier: 0.1702 (0.1702)  loss_box_reg: 0.2426 (0.2426)  loss_objectness: 0.0051 (0.0051)  loss_rpn_box_reg: 0.0563 (0.0563)  time: 2.5737  data: 1.3454  max mem: 5600\n",
            "Epoch: [7]  [1/2]  eta: 0:00:02  lr: 0.000013  loss: 0.4742 (0.4817)  loss_classifier: 0.1612 (0.1657)  loss_box_reg: 0.2161 (0.2294)  loss_objectness: 0.0033 (0.0042)  loss_rpn_box_reg: 0.0563 (0.0824)  time: 2.4058  data: 1.1755  max mem: 5600\n",
            "Epoch: [7] Total time: 0:00:04 (2.4067 s / it)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [00:01<00:01,  1.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [0/2]  eta: 0:00:03    time: 1.5131  data: 0.9372  max mem: 5600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:03<00:00,  1.58s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [1/2]  eta: 0:00:01    time: 1.5762  data: 0.9840  max mem: 5600\n",
            "Test: Total time: 0:00:03 (1.5771 s / it)\n",
            "\n",
            "\n",
            "==== Validation Results (mAP@50-95) ====\n",
            "mAP50-95: 0.227\n",
            "\n",
            "Per Class mAP (0.50:0.95 IoU threshold):\n",
            "class1: 0.015\n",
            "class2: 0.214\n",
            "class3: 0.418\n",
            "class4: 0.262\n",
            "\n",
            "\n",
            "==== Validation Results (mAP@50) ====\n",
            "mAP@50: 0.398\n",
            "\n",
            "Per Class mAP (50% IoU threshold):\n",
            "class1: 0.083\n",
            "class2: 0.286\n",
            "class3: 0.768\n",
            "class4: 0.454\n",
            "\n",
            "\n",
            "Epoch: [8]  [0/2]  eta: 0:00:04  lr: 0.000013  loss: 0.4787 (0.4787)  loss_classifier: 0.1731 (0.1731)  loss_box_reg: 0.2488 (0.2488)  loss_objectness: 0.0029 (0.0029)  loss_rpn_box_reg: 0.0539 (0.0539)  time: 2.1656  data: 0.9484  max mem: 5600\n",
            "Epoch: [8]  [1/2]  eta: 0:00:02  lr: 0.000013  loss: 0.4787 (0.4893)  loss_classifier: 0.1601 (0.1666)  loss_box_reg: 0.2262 (0.2375)  loss_objectness: 0.0024 (0.0027)  loss_rpn_box_reg: 0.0539 (0.0826)  time: 2.2305  data: 0.9991  max mem: 5600\n",
            "Epoch: [8] Total time: 0:00:04 (2.2317 s / it)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [00:01<00:01,  1.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [0/2]  eta: 0:00:03    time: 1.9241  data: 1.3405  max mem: 5600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:03<00:00,  1.75s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [1/2]  eta: 0:00:01    time: 1.7530  data: 1.1572  max mem: 5600\n",
            "Test: Total time: 0:00:03 (1.7543 s / it)\n",
            "\n",
            "\n",
            "==== Validation Results (mAP@50-95) ====\n",
            "mAP50-95: 0.195\n",
            "\n",
            "Per Class mAP (0.50:0.95 IoU threshold):\n",
            "class1: 0.020\n",
            "class2: 0.175\n",
            "class3: 0.328\n",
            "class4: 0.256\n",
            "\n",
            "\n",
            "==== Validation Results (mAP@50) ====\n",
            "mAP@50: 0.417\n",
            "\n",
            "Per Class mAP (50% IoU threshold):\n",
            "class1: 0.083\n",
            "class2: 0.333\n",
            "class3: 0.745\n",
            "class4: 0.508\n",
            "\n",
            "\n",
            "Epoch: [9]  [0/2]  eta: 0:00:04  lr: 0.000013  loss: 0.4812 (0.4812)  loss_classifier: 0.1780 (0.1780)  loss_box_reg: 0.2499 (0.2499)  loss_objectness: 0.0009 (0.0009)  loss_rpn_box_reg: 0.0524 (0.0524)  time: 2.1808  data: 0.9704  max mem: 5600\n",
            "Epoch: [9]  [1/2]  eta: 0:00:02  lr: 0.000006  loss: 0.4812 (0.4879)  loss_classifier: 0.1574 (0.1677)  loss_box_reg: 0.2278 (0.2389)  loss_objectness: 0.0009 (0.0023)  loss_rpn_box_reg: 0.0524 (0.0791)  time: 2.2292  data: 1.0105  max mem: 5600\n",
            "Epoch: [9] Total time: 0:00:04 (2.2303 s / it)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [00:01<00:01,  1.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [0/2]  eta: 0:00:03    time: 1.5167  data: 0.9456  max mem: 5600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:03<00:00,  1.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [1/2]  eta: 0:00:01    time: 1.5514  data: 0.9623  max mem: 5600\n",
            "Test: Total time: 0:00:03 (1.5528 s / it)\n",
            "\n",
            "\n",
            "==== Validation Results (mAP@50-95) ====\n",
            "mAP50-95: 0.222\n",
            "\n",
            "Per Class mAP (0.50:0.95 IoU threshold):\n",
            "class1: 0.020\n",
            "class2: 0.210\n",
            "class3: 0.315\n",
            "class4: 0.343\n",
            "\n",
            "\n",
            "==== Validation Results (mAP@50) ====\n",
            "mAP@50: 0.482\n",
            "\n",
            "Per Class mAP (50% IoU threshold):\n",
            "class1: 0.067\n",
            "class2: 0.400\n",
            "class3: 0.708\n",
            "class4: 0.751\n",
            "\n",
            "\n",
            "Dummy Training complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "img_dir = \"\"\n",
        "ann_file = \"\"\n",
        "full_dataset = get_coco_dataset(img_dir, ann_file)\n",
        "train_dataset, test_dataset = split_dataset(full_dataset)\n",
        "train_loader, test_loader = create_dataloader(train_dataset, test_dataset)\n",
        "\n",
        "num_classes = 0 # background + num of classes\n",
        "model = get_model(num_classes)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "optimizer, lr_scheduler = setup_optimizer_scheduler(model)\n",
        "\n",
        "CLASSES = []  # Dont include background class\n",
        "\n",
        "train_and_evaluate(model, test_loader, test_loader, optimizer, lr_scheduler, num_epochs=10, device=device, CLASSES=CLASSES)\n",
        "\n",
        "print(\"Training complete!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "7n14jDBT3rvE",
        "Aox0w73_AuiO",
        "ACno859LqtAx",
        "QGTEbJu2qQIP",
        "qq8BkY83Z62J",
        "k8F1E8kdr3ua",
        "EYy8EPBIskgB",
        "26YnKqODuEF-",
        "v4ag1G-Fn_Gi"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
